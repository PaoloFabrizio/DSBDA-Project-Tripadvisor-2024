{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYjFJ5pnLwfFrWBSqM+DJu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ab1hMzT2U4Cv"},"outputs":[],"source":["import findspark\n","findspark.init()\n","import pyspark\n","from pyspark.sql import SparkSession, Row\n","import re\n","\n","# preprocessing to handle fields with more words separated by \",\"\n","def preprocess_csv(input_path, output_path):\n","    with open(input_path, 'r') as file:\n","        lines = file.readlines()\n","\n","    processed_lines = []\n","    for line in lines:\n","        line = re.sub(r'\\[([^\\]]+)\\]', lambda m: m.group(0).replace(',', ';'), line)\n","        line = re.sub(r'\"([^\"]*)\"', lambda m: m.group(0).replace(',', ';'), line)\n","        processed_lines.append(line)\n","\n","    with open(output_path, 'w') as file:\n","        file.writelines(processed_lines)\n","\n","# Context with Hadoop\n","'''\n","spark = SparkSession.builder \\\n","    .appName(\"CSV to HDFS\") \\\n","    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode_host:54310\") \\\n","    .getOrCreate()\n","'''\n","\n","# Context without Hadoop\n","sc = pyspark.SparkContext(appName=\"CSV Reader\")\n","spark = SparkSession(sc)\n","\n","def drop_columns_and_save(df, columns_to_drop, num_partitions):\n","    rdd = df.rdd\n","\n","    header = df.columns\n","    columns_to_keep = [i for i in range(len(header)) if i not in columns_to_drop]\n","    rdd_filtered = rdd.map(lambda row: tuple(row[i] for i in columns_to_keep))\n","\n","    # First column is 0, 1, 2...\n","    rdd_with_index = rdd_filtered.zipWithIndex().map(lambda row: (row[1],) + row[0])\n","    new_header = ['index'] + [header[i] for i in columns_to_keep]\n","    df_filtered = rdd_with_index.map(lambda x: Row(*x)).toDF(new_header)\n","\n","    rdd_repartitioned = df_filtered.rdd.repartition(num_partitions)\n","    df_repartitioned = rdd_repartitioned.map(lambda x: Row(*x)).toDF(new_header)\n","\n","    # Save locally since Hadoop doesn't work\n","    df_repartitioned.write.csv(\"tripadvisor_filtered.csv\", header=True, mode='overwrite')\n","\n","    '''\n","    hdfs_path = \"hdfs://namenode_host:54310/projectDSBDA/dataset1.csv\"\n","    df_filtered.write.csv(hdfs_path, header=True, mode='overwrite')\n","    '''\n","\n","    return df_repartitioned\n","\n","def print_shape(dataframe):\n","    num_rows = dataframe.count()\n","    num_columns = len(dataframe.columns)\n","    print(f\"Shape of the DataFrame: ({num_rows}, {num_columns})\")\n","\n","#----------MAIN----------\n","\n","# Preprocess the CSV file\n","preprocess_csv('tripadvisor.csv', 'tripadvisor_processed.csv')\n","\n","# Read the preprocessed CSV file\n","df = spark.read.csv(\"tripadvisor_processed.csv\", header=True, inferSchema=True, quote='\"', escape='\"', multiLine=True)\n","\n","print_shape(df)\n","\n","# divide in partitions otherwise too heavy the csv to handle (more or less 200000 rows for partition)\n","num_partitions = round(df.count() / 200000)\n","\n","# Drop columns (give number)\n","df_filtered = drop_columns_and_save(df, [], num_partitions)\n","\n","# Final shape is initial columns count - elements in the list + 1 (for the index column)\n","print_shape(df_filtered)\n","\n","spark.stop()"]}]}