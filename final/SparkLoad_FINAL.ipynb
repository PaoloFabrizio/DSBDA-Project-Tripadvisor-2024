{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ad48a8-b97e-4137-b9e2-54f5a0b8079f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (100000, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (100000, 14)\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import re\n",
    "\n",
    "def preprocess_csv(input_path, output_path):\n",
    "    with open(input_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "   \n",
    "    processed_lines = []\n",
    "    for line in lines:\n",
    "        line = re.sub(r'\\[([^\\]]+)\\]', lambda m: m.group(0).replace(',', ';'), line)\n",
    "        line = re.sub(r'\"([^\"]*)\"', lambda m: m.group(0).replace(',', ';'), line)\n",
    "\n",
    "\n",
    "        processed_lines.append(line)\n",
    "   \n",
    "    with open(output_path, 'w') as file:\n",
    "        file.writelines(processed_lines)\n",
    "\n",
    "# Context with Hadoop\n",
    "'''\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV to HDFS\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode_host:54310\") \\\n",
    "    .getOrCreate()\n",
    "'''\n",
    "\n",
    "# Context without Hadoop\n",
    "sc = pyspark.SparkContext(appName=\"CSV Reader\")\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "def drop_columns_and_save(df, columns_to_drop, num_partitions):\n",
    "    rdd = df.rdd\n",
    "\n",
    "    header = df.columns\n",
    "    columns_to_keep = [i for i in range(len(header)) if i not in columns_to_drop]\n",
    "    rdd_filtered = rdd.map(lambda row: tuple(row[i] for i in columns_to_keep))\n",
    "\n",
    "    # First column is 0, 1, 2...\n",
    "    rdd_with_index = rdd_filtered.zipWithIndex().map(lambda row: (row[1],) + row[0])\n",
    "    new_header = ['index'] + [header[i] for i in columns_to_keep]\n",
    "    df_filtered = rdd_with_index.map(lambda x: Row(*x)).toDF(new_header)\n",
    "\n",
    "    rdd_repartitioned = df_filtered.rdd.repartition(num_partitions)\n",
    "    df_repartitioned = rdd_repartitioned.map(lambda x: Row(*x)).toDF(new_header)\n",
    "\n",
    "    # Save locally since Hadoop doesn't work\n",
    "    df_repartitioned.write.csv(\"tripadvisor_filtered\", header=True, mode='overwrite')\n",
    "   \n",
    "    '''\n",
    "    hdfs_path = \"hdfs://namenode_host:54310/projectDSBDA/dataset1.csv\"\n",
    "    df_filtered.write.csv(hdfs_path, header=True, mode='overwrite')\n",
    "    '''\n",
    "\n",
    "    return df_repartitioned\n",
    "\n",
    "def print_shape(dataframe):\n",
    "    num_rows = dataframe.count()\n",
    "    num_columns = len(dataframe.columns)\n",
    "    print(f\"Shape of the DataFrame: ({num_rows}, {num_columns})\")\n",
    "\n",
    "#----------MAIN----------\n",
    "\n",
    "# Preprocess the CSV file\n",
    "preprocess_csv('tripadvisor_new.csv', 'tripadvisor_processed.csv')\n",
    "\n",
    "# Read the preprocessed CSV file\n",
    "df = spark.read.csv(\"tripadvisor_processed.csv\", header=True, inferSchema=True, quote='\"', escape='\"', multiLine=True)\n",
    "\n",
    "print_shape(df)\n",
    "\n",
    "# divide in partitions otherwise too heavy the csv to handle (more or less 200000 rows for partition)\n",
    "num_partitions = round(df.count() / 100000)\n",
    "\n",
    "# Drop columns (give number)\n",
    "df_filtered = drop_columns_and_save(df, [0, 2, 4, 7, 8, 9, 10, 12, 13, 14, 15, 17, 19, 24, 25, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41], num_partitions)\n",
    "\n",
    "# Final shape is initial columns count - elements in the list + 1 (for the index column)\n",
    "print_shape(df_filtered)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b9e47e-438b-44c9-ac3e-bdaa3a5b6528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
